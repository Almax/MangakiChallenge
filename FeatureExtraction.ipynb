{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = \"/mnt/d/Data/mangaki-data-challenge/\"\n",
    "\n",
    "record = pd.read_csv(data+'watched.csv', dtype={\n",
    "    'user_id': np.int16,\n",
    "    'work_id': np.int16,\n",
    "    'rating': 'category'\n",
    "})\n",
    "\n",
    "train_full = pd.read_csv(data+'train_withcv.csv', dtype={\n",
    "    'user_id': np.int16,\n",
    "    'work_id': np.int16,\n",
    "    'rating': np.int8,\n",
    "    'cv': np.int8\n",
    "})\n",
    "\n",
    "test = pd.read_csv(data+'test.csv', dtype={\n",
    "    'user_id': np.int16,\n",
    "    'work_id': np.int16\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = pd.crosstab(record.user_id, record.rating).add_prefix('user_').apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratelut = {\n",
    "    'dislike':1.0,\n",
    "    'like':3.0,\n",
    "    'love':4.0,\n",
    "    'neutral':2.0\n",
    "}\n",
    "record['score']=record.rating.map(lambda x: ratelut[x]).astype(float)\n",
    "u2 = record[['user_id', 'score']].groupby(by='user_id')['score'].agg(['mean', 'std']).rename(columns={'mean':'user_mean', 'std':'user_std'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_full = train\n",
    "train=train_full[train_full.cv==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u3 = pd.crosstab(train.user_id, train.rating).add_prefix('user_').apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = pd.crosstab(record.work_id, record.rating).add_prefix('work_').apply(lambda r: r/r.sum(), axis=1)\n",
    "i2 = record[['work_id', 'score']].groupby(by='work_id')['score'].agg(['mean', 'std']).add_prefix('item_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_full[train_full.cv==3]\n",
    "valid = train_full[train_full.cv.isin([1,2])]\n",
    "u3 = pd.crosstab(train.user_id, train.rating).add_prefix('user_').apply(lambda r: r/r.sum(), axis=1)\n",
    "i3 = pd.crosstab(train.work_id, train.rating).add_prefix('item_').apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(u3, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "merge(i2, left_on='work_id', right_index=True, how='left').\\\n",
    "merge(i3, left_on='work_id', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['user_id', 'work_id', 'cv'], axis=1)\n",
    "train.to_csv(data+\"train_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = valid.merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(u3, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "merge(i2, left_on='work_id', right_index=True, how='left').\\\n",
    "merge(i3, left_on='work_id', right_index=True, how='left')\n",
    "valid = valid.drop(['user_id', 'work_id', 'cv'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.to_csv(data+\"valid_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lgb.Dataset(train.drop('rating', axis=1), train['rating'])\n",
    "V = lgb.Dataset(valid.drop('rating', axis=1), valid['rating'], reference=X)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'auc'},\n",
    "    'num_leaves': 20,\n",
    "    'min_data_in_leaf': 800,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 1,\n",
    "    'verbose': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.861324\tvalid_1's auc: 0.685332\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\ttraining's auc: 0.900989\tvalid_1's auc: 0.694957\n",
      "[3]\ttraining's auc: 0.909735\tvalid_1's auc: 0.727588\n",
      "[4]\ttraining's auc: 0.908671\tvalid_1's auc: 0.746335\n",
      "[5]\ttraining's auc: 0.919939\tvalid_1's auc: 0.757441\n",
      "[6]\ttraining's auc: 0.928101\tvalid_1's auc: 0.761275\n",
      "[7]\ttraining's auc: 0.928261\tvalid_1's auc: 0.765559\n",
      "[8]\ttraining's auc: 0.930713\tvalid_1's auc: 0.768853\n",
      "[9]\ttraining's auc: 0.938168\tvalid_1's auc: 0.769829\n",
      "[10]\ttraining's auc: 0.941206\tvalid_1's auc: 0.767945\n",
      "[11]\ttraining's auc: 0.94848\tvalid_1's auc: 0.764205\n",
      "[12]\ttraining's auc: 0.949029\tvalid_1's auc: 0.765623\n",
      "[13]\ttraining's auc: 0.949092\tvalid_1's auc: 0.767825\n",
      "[14]\ttraining's auc: 0.951325\tvalid_1's auc: 0.763372\n",
      "[15]\ttraining's auc: 0.954505\tvalid_1's auc: 0.757294\n",
      "[16]\ttraining's auc: 0.955919\tvalid_1's auc: 0.754244\n",
      "[17]\ttraining's auc: 0.955879\tvalid_1's auc: 0.755022\n",
      "[18]\ttraining's auc: 0.957224\tvalid_1's auc: 0.752914\n",
      "[19]\ttraining's auc: 0.958035\tvalid_1's auc: 0.748683\n",
      "[20]\ttraining's auc: 0.958722\tvalid_1's auc: 0.746239\n",
      "[21]\ttraining's auc: 0.959036\tvalid_1's auc: 0.746254\n",
      "[22]\ttraining's auc: 0.959455\tvalid_1's auc: 0.740495\n",
      "[23]\ttraining's auc: 0.959558\tvalid_1's auc: 0.737914\n",
      "[24]\ttraining's auc: 0.960625\tvalid_1's auc: 0.735883\n",
      "[25]\ttraining's auc: 0.960766\tvalid_1's auc: 0.734879\n",
      "[26]\ttraining's auc: 0.96164\tvalid_1's auc: 0.734475\n",
      "[27]\ttraining's auc: 0.96183\tvalid_1's auc: 0.734829\n",
      "[28]\ttraining's auc: 0.962055\tvalid_1's auc: 0.73386\n",
      "[29]\ttraining's auc: 0.962141\tvalid_1's auc: 0.733257\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.938168\tvalid_1's auc: 0.769829\n"
     ]
    }
   ],
   "source": [
    "gbdt = lgb.train(params, X, valid_sets=[X,V],early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
