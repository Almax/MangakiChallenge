{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = \"/mnt/d/Data/mangaki-data-challenge/\"\n",
    "\n",
    "record = pd.read_csv(data+'watched.csv', dtype={\n",
    "    'user_id': np.int16,\n",
    "    'work_id': np.int16,\n",
    "    'rating': 'category'\n",
    "})\n",
    "\n",
    "train_full = pd.read_csv(data+'train_withcv.csv', dtype={\n",
    "    'user_id': np.int16,\n",
    "    'work_id': np.int16,\n",
    "    'rating': np.int8,\n",
    "    'cv': np.int8\n",
    "})\n",
    "\n",
    "test = pd.read_csv(data+'test.csv', dtype={\n",
    "    'user_id': np.int16,\n",
    "    'work_id': np.int16\n",
    "})\n",
    "\n",
    "title = pd.read_csv(data+'titles.csv', dtype={\n",
    "    'work_id': np.int16,\n",
    "    'title': str,\n",
    "    'category': str\n",
    "})\n",
    "catlut = {'anime': 0, 'manga': 1, 'album':2}\n",
    "ratelut = {'dislike':1.0, 'like':3.0, 'love':4.0, 'neutral':2.0}\n",
    "title = title[['work_id', 'category']].set_index('work_id').applymap(lambda x: catlut[x])\n",
    "record = record.merge(title, left_on='work_id', right_index=True)\n",
    "record['score']=record.rating.map(lambda x: ratelut[x]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u1 = pd.crosstab(record[record.category==0].user_id, record[record.category==0].rating).add_prefix('user_anime_').apply(lambda r: r/r.sum(), axis=1)\n",
    "u2 = pd.crosstab(record[record.category==1].user_id, record[record.category==1].rating).add_prefix('user_manga_').apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u3 = record[record.category==0][['user_id', 'score']].groupby(by='user_id')['score'].agg(['mean', 'std']).rename(columns={'mean':'user_anime_mean', 'std':'user_anime_std'})\n",
    "u4 = record[record.category==1][['user_id', 'score']].groupby(by='user_id')['score'].agg(['mean', 'std']).rename(columns={'mean':'user_manga_mean', 'std':'user_manga_std'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1 = pd.crosstab(record.work_id, record.rating).add_prefix('work_').apply(lambda r: r/r.sum(), axis=1)\n",
    "i2 = record[['work_id', 'score']].groupby(by='work_id')['score'].agg(['mean', 'std']).add_prefix('item_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cv in [1,2,3]:\n",
    "    valid = train_full[train_full.cv==cv]\n",
    "    train = train_full[~train_full.cv.isin([cv])]\n",
    "    train = train.merge(title.add_prefix('item_'), left_on='work_id', right_index=True, how='left').\\\n",
    "                  merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u3, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u4, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "                  merge(i2, left_on='work_id', right_index=True, how='left')\n",
    "    valid = valid.merge(title.add_prefix('item_'), left_on='work_id', right_index=True, how='left').\\\n",
    "                  merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u3, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u4, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "                  merge(i2, left_on='work_id', right_index=True, how='left')\n",
    "    train = train.drop(['cv'], axis=1)\n",
    "    train.to_csv(data+\"baseline/train_{0}.csv\".format(cv), index=False)\n",
    "    valid = valid.drop(['cv'], axis=1)\n",
    "    valid.to_csv(data+\"baseline/valid_{0}.csv\".format(cv), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_full\n",
    "train = train.merge(title.add_prefix('item_'), left_on='work_id', right_index=True, how='left').\\\n",
    "                  merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u3, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u4, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "                  merge(i2, left_on='work_id', right_index=True, how='left')\n",
    "                  \n",
    "train = train.drop(['cv'], axis=1)\n",
    "train.to_csv(data+\"baseline/train_0.csv\", index=False)\n",
    "\n",
    "test = test.merge(title.add_prefix('item_'), left_on='work_id', right_index=True, how='left').\\\n",
    "                  merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u3, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u4, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "                  merge(i2, left_on='work_id', right_index=True, how='left')\n",
    "                  \n",
    "test.to_csv(data+\"baseline/test_0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "W2V feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2vdim=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from random import sample\n",
    "\n",
    "record_positive = record[record.rating.isin(['like', 'love'])]\n",
    "record_positive_effectiveuser = record_positive[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "record_positive_effectiveuser = record_positive_effectiveuser[record_positive_effectiveuser.work_count>5]\n",
    "record_positive = record_positive[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "record_positive = pd.concat((record_positive, record_positive_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_positive.iterrows():\n",
    "    t = ['i{0}'.format(str(i)) for i in itm[1].work_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "item_w2v_positive = []\n",
    "item_index = []\n",
    "for item in range(9897):\n",
    "    if 'i{0}'.format(item) in w2v_positive:\n",
    "        item_index.append(item)\n",
    "        item_w2v_positive.append(w2v_positive['i{0}'.format(item)])\n",
    "item_w2v_positive = np.vstack(item_w2v_positive)\n",
    "i4 = pd.DataFrame(data = item_w2v_positive, index=item_index, columns=['itemw2vpos_{0}'.format(i) for i in range(w2vdim)])\n",
    "i4.index.name='work_id'\n",
    "\n",
    "i4.reset_index().to_csv(data+\"features/item_w2v_shuffled_{0}d.csv\".format(w2vdim), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for user\n",
    "record_positive = record[record.rating.isin(['like', 'love'])]\n",
    "record_positive_effectiveitem = record_positive[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "record_positive_effectiveitem = record_positive_effectiveitem[record_positive_effectiveitem.user_count>5]\n",
    "record_positive = record_positive[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "record_positive = pd.concat((record_positive, record_positive_effectiveitem), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_positive.iterrows():\n",
    "    t = ['u{0}'.format(str(i)) for i in itm[1].user_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "user_w2v_positive = []\n",
    "user_index = []\n",
    "for user in range(1983):\n",
    "    if 'u{0}'.format(user) in w2v_positive:\n",
    "        user_index.append(user)\n",
    "        user_w2v_positive.append(w2v_positive['u{0}'.format(user)])\n",
    "user_w2v_positive = np.vstack(user_w2v_positive)\n",
    "u4 = pd.DataFrame(data = user_w2v_positive, index=user_index, columns=['userw2vpos_{0}'.format(i) for i in range(w2vdim)])\n",
    "u4.index.name='user_id'\n",
    "\n",
    "u4.reset_index().to_csv(data+\"features/user_w2v_shuffled_{0}d.csv\".format(w2vdim), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from random import sample\n",
    "\n",
    "record_negative = record[record.rating.isin(['dislike'])]\n",
    "record_negative_effectiveuser = record_negative[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "record_negative_effectiveuser = record_negative_effectiveuser[record_negative_effectiveuser.work_count>5]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "record_negative = pd.concat((record_negative, record_negative_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    t = ['i{0}'.format(str(i)) for i in itm[1].work_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_negative = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "item_w2v_negative = []\n",
    "item_index = []\n",
    "for item in range(9897):\n",
    "    if 'i{0}'.format(item) in w2v_negative:\n",
    "        item_index.append(item)\n",
    "        item_w2v_negative.append(w2v_negative['i{0}'.format(item)])\n",
    "item_w2v_negative = np.vstack(item_w2v_negative)\n",
    "i4 = pd.DataFrame(data = item_w2v_negative, index=item_index, columns=['itemw2vneg_{0}'.format(i) for i in range(w2vdim)])\n",
    "i4.index.name='work_id'\n",
    "\n",
    "i4.reset_index().to_csv(data+\"features/item_w2vneg_shuffled_{0}d.csv\".format(w2vdim), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_negative = record[record.rating.isin(['dislike'])]\n",
    "record_negative_effectiveitem = record_negative[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "record_negative_effectiveitem = record_negative_effectiveitem[record_negative_effectiveitem.user_count>5]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "record_negative = pd.concat((record_negative, record_negative_effectiveitem), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    t = ['u{0}'.format(str(i)) for i in itm[1].user_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_negative = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "user_w2v_negative = []\n",
    "user_index = []\n",
    "for user in range(1983):\n",
    "    if 'u{0}'.format(user) in w2v_negative:\n",
    "        user_index.append(user)\n",
    "        user_w2v_negative.append(w2v_negative['u{0}'.format(user)])\n",
    "user_w2v_negative = np.vstack(user_w2v_negative)\n",
    "u4 = pd.DataFrame(data = user_w2v_negative, index=user_index, columns=['userw2vneg_{0}'.format(i) for i in range(w2vdim)])\n",
    "u4.index.name='user_id'\n",
    "\n",
    "u4.reset_index().to_csv(data+\"features/user_w2vneg_shuffled_{0}d.csv\".format(w2vdim), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LDA feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "record_positive = record[record.rating.isin(['like', 'love', 'neutral'])]\n",
    "record_positive_effectiveuser = record_positive[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "record_positive_effectiveuser = record_positive_effectiveuser[record_positive_effectiveuser.work_count>5]\n",
    "record_positive = record_positive[['user_id', 'work_id', 'rating']].groupby(by='user_id').agg(lambda x: list(x))\n",
    "record_positive = pd.concat((record_positive, record_positive_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "corpus = []\n",
    "dictionary = {}\n",
    "cnt=0\n",
    "ratlut = {'love': 1.5, 'like': 1, 'neutral': 0.5}\n",
    "for itm in record_positive.iterrows():\n",
    "    for work in itm[1].work_id:\n",
    "        strwork = 'i{0}'.format(work)\n",
    "        if strwork not in dictionary:\n",
    "            dictionary[strwork]=cnt\n",
    "            cnt+=1\n",
    "    corpus.append([(dictionary['i{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].work_id, itm[1].rating)])\n",
    "invdict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "lda_user = LdaModel(corpus=corpus, num_topics=num_topics, id2word=invdict)\n",
    "\n",
    "record_positive = record[record.rating.isin(['like', 'love', 'neutral'])]\n",
    "record_positive = record_positive[['user_id', 'work_id', 'rating']].groupby(by='user_id').agg(lambda x: list(x))\n",
    "\n",
    "corpus = []\n",
    "for itm in record_positive.iterrows():\n",
    "    corpus.append([(dictionary['i{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].work_id, itm[1].rating) if 'i{0}'.format(i) in dictionary])\n",
    "\n",
    "user_lda_positive = []\n",
    "for i, topic in enumerate(lda_user[corpus]):\n",
    "    temp = np.zeros(num_topics)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    user_lda_positive.append(temp)\n",
    "user_lda_positive = np.vstack(user_lda_positive)\n",
    "u5 = pd.DataFrame(data = user_lda_positive, index=record_positive.index, columns=['user_ldapos_{0}'.format(i) for i in range(num_topics)])\n",
    "\n",
    "u5.reset_index().to_csv(data+\"features/user_lda_{0}d.csv\".format(num_topics), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_positive = record[record.rating.isin(['like', 'love', 'neutral'])]\n",
    "record_positive_effectiveitem = record_positive[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "record_positive_effectiveitem = record_positive_effectiveitem[record_positive_effectiveitem.user_count>5]\n",
    "record_positive = record_positive[['user_id', 'work_id', 'rating']].groupby(by='work_id').agg(lambda x: list(x))\n",
    "record_positive = pd.concat((record_positive, record_positive_effectiveitem), axis=1, join='inner')\n",
    "\n",
    "corpus = []\n",
    "dictionary = {}\n",
    "cnt=0\n",
    "ratlut = {'love': 1.5, 'like': 1, 'neutral': 0.5}\n",
    "for itm in record_positive.iterrows():\n",
    "    for user in itm[1].user_id:\n",
    "        struser = 'u{0}'.format(user)\n",
    "        if struser not in dictionary:\n",
    "            dictionary[struser]=cnt\n",
    "            cnt+=1\n",
    "    corpus.append([(dictionary['u{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].user_id, itm[1].rating)])\n",
    "invdict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "lda_item = LdaModel(corpus=corpus, num_topics=num_topics, id2word=invdict)\n",
    "\n",
    "record_positive = record[record.rating.isin(['like', 'love', 'neutral'])]\n",
    "record_positive = record_positive[['user_id', 'work_id', 'rating']].groupby(by='work_id').agg(lambda x: list(x))\n",
    "\n",
    "corpus = []\n",
    "for itm in record_positive.iterrows():\n",
    "    corpus.append([(dictionary['u{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].user_id, itm[1].rating) if 'u{0}'.format(i) in dictionary])\n",
    "\n",
    "item_lda_positive = []\n",
    "for i, topic in enumerate(lda_item[corpus]):\n",
    "    temp = np.zeros(num_topics)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    item_lda_positive.append(temp)\n",
    "item_lda_positive = np.vstack(item_lda_positive)\n",
    "i5 = pd.DataFrame(data = item_lda_positive, index=record_positive.index, columns=['item_ldapos_{0}'.format(i) for i in range(num_topics)])\n",
    "\n",
    "i5.reset_index().to_csv(data+\"features/item_lda_{0}d.csv\".format(num_topics), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "\n",
    "record_negative = record[record.rating.isin(['dislike'])]\n",
    "record_negative_effectiveuser = record_negative[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "record_negative_effectiveuser = record_negative_effectiveuser[record_negative_effectiveuser.work_count>5]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "record_negative = pd.concat((record_negative, record_negative_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    sentences.append(['i{0}'.format(str(i)) for i in itm[1].work_list])\n",
    "    \n",
    "dictionary = corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(rc) for rc in sentences]\n",
    "\n",
    "lda_user = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "record_negative = record[record.rating.isin(['dislike'])]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    sentences.append(['i{0}'.format(str(i)) for i in itm[1].work_list])\n",
    "corpus = [dictionary.doc2bow(rc) for rc in sentences]\n",
    "\n",
    "user_lda_negative = []\n",
    "for i, topic in enumerate(lda_user[corpus]):\n",
    "    temp = np.zeros(num_topics)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    user_lda_negative.append(temp)\n",
    "user_lda_negative = np.vstack(user_lda_negative)\n",
    "u5 = pd.DataFrame(data = user_lda_negative, index=record_negative.index, columns=['user_ldapos_{0}'.format(i) for i in range(num_topics)])\n",
    "\n",
    "u5.reset_index().to_csv(data+\"features/user_ldaneg_{0}d.csv\".format(num_topics), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_negative = record[record.rating.isin(['dislike'])]\n",
    "record_negative_effectiveitem = record_negative[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "record_negative_effectiveitem = record_negative_effectiveitem[record_negative_effectiveitem.user_count>5]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "record_negative = pd.concat((record_negative, record_negative_effectiveitem), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    sentences.append(['u{0}'.format(str(i)) for i in itm[1].user_list])\n",
    "    \n",
    "dictionary = corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(rc) for rc in sentences]\n",
    "\n",
    "lda_item = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "record_negative = record[record.rating.isin(['like', 'love'])]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    sentences.append(['u{0}'.format(str(i)) for i in itm[1].user_list])\n",
    "corpus = [dictionary.doc2bow(rc) for rc in sentences]\n",
    "\n",
    "item_lda_negative = []\n",
    "for i, topic in enumerate(lda_item[corpus]):\n",
    "    temp = np.zeros(num_topics)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    item_lda_negative.append(temp)\n",
    "item_lda_negative = np.vstack(item_lda_negative)\n",
    "i5 = pd.DataFrame(data = item_lda_negative, index=record_negative.index, columns=['item_ldapos_{0}'.format(i) for i in range(num_topics)])\n",
    "\n",
    "i5.reset_index().to_csv(data+\"features/item_ldaneg_{0}d.csv\".format(num_topics), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "convert to libfm format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "\n",
    "path = \"/mnt/d/Data/mangaki-data-challenge/\"\n",
    "\n",
    "train = pd.read_csv(path+\"latest/train_0.csv\")\n",
    "valid = pd.read_csv(path+\"latest/test_0.csv\")\n",
    "\n",
    "data = []\n",
    "row = []\n",
    "col = []\n",
    "for i, itm in enumerate(train.iterrows()):\n",
    "    row.append(i)\n",
    "    col.append(itm[1].user_id)\n",
    "    data.append(1.0)\n",
    "    row.append(i)\n",
    "    col.append(itm[1].work_id+1983)\n",
    "    data.append(1.0)\n",
    "    row.append(i)\n",
    "    col.append(itm[1].item_category+1983+9897)\n",
    "    data.append(1.0)\n",
    "    \n",
    "basic = coo_matrix((data, (row, col)), shape=(train.shape[0], 1983+9897+3))\n",
    "ext = train.ix[:, 4:].fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext_prob = np.hstack([ext[:, 0:8], ext[:, 12:16], ext[:, 102:142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext_nonprob = np.hstack([ext[:, 8:12], ext[:, 16:102], ext[:, 142:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "normalizer = MaxAbsScaler()\n",
    "ext = normalizer.fit_transform(ext_nonprob)\n",
    "\n",
    "value = hstack([basic, ext])\n",
    "dump_svmlight_file(value, train.rating.values, path+\"libfm/train_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "\n",
    "path = \"/mnt/d/Data/mangaki-data-challenge/\"\n",
    "for cv in [1,2,3]:\n",
    "    train = pd.read_csv(path+\"latest/train_{0}.csv\".format(cv))\n",
    "    valid = pd.read_csv(path+\"latest/valid_{0}.csv\".format(cv))\n",
    "    \n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    for i, itm in enumerate(train.iterrows()):\n",
    "        row.append(i)\n",
    "        col.append(itm[1].user_id)\n",
    "        data.append(1.0)\n",
    "        row.append(i)\n",
    "        col.append(itm[1].work_id+1983)\n",
    "        data.append(1.0)\n",
    "        row.append(i)\n",
    "        col.append(itm[1].item_category+1983+9897)\n",
    "        data.append(1.0)\n",
    "\n",
    "    basic = coo_matrix((data, (row, col)), shape=(train.shape[0], 1983+9897+3))\n",
    "    ext = train.ix[:, 4:].fillna(0).values\n",
    "    ext_prob = np.hstack([ext[:, 0:8], ext[:, 12:16], ext[:, 102:142]])\n",
    "    ext_nonprob = np.hstack([ext[:, 8:12], ext[:, 16:102], ext[:, 142:]])\n",
    "    normalizer = MaxAbsScaler()\n",
    "    ext_nonprob = normalizer.fit_transform(ext_nonprob)\n",
    "    value = hstack([basic, ext_prob, ext_nonprob])\n",
    "    dump_svmlight_file(value, train.rating.values, path+\"libfm/train_{0}.csv\".format(cv))\n",
    "    \n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    for i, itm in enumerate(valid.iterrows()):\n",
    "        row.append(i)\n",
    "        col.append(itm[1].user_id)\n",
    "        data.append(1.0)\n",
    "        row.append(i)\n",
    "        col.append(itm[1].work_id+1983)\n",
    "        data.append(1.0)\n",
    "        row.append(i)\n",
    "        col.append(itm[1].item_category+1983+9897)\n",
    "        data.append(1.0)\n",
    "\n",
    "    basic = coo_matrix((data, (row, col)), shape=(valid.shape[0], 1983+9897+3))\n",
    "    ext = valid.ix[:, 4:].fillna(0).values\n",
    "    ext_prob = np.hstack([ext[:, 0:8], ext[:, 12:16], ext[:, 102:142]])\n",
    "    ext_nonprob = np.hstack([ext[:, 8:12], ext[:, 16:102], ext[:, 142:]])\n",
    "    ext_nonprob = normalizer.transform(ext_nonprob)\n",
    "    value = hstack([basic, ext_prob, ext_nonprob])\n",
    "    dump_svmlight_file(value, valid.rating.values, path+\"libfm/valid_{0}.csv\".format(cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "\n",
    "path = \"/mnt/d/Data/mangaki-data-challenge/\"\n",
    "\n",
    "train = pd.read_csv(path+\"latest/train_0.csv\")\n",
    "valid = pd.read_csv(path+\"latest/test_0.csv\")\n",
    "\n",
    "data = []\n",
    "row = []\n",
    "col = []\n",
    "for i, itm in enumerate(train.iterrows()):\n",
    "    row.append(i)\n",
    "    col.append(itm[1].user_id)\n",
    "    data.append(1.0)\n",
    "    row.append(i)\n",
    "    col.append(itm[1].work_id+1983)\n",
    "    data.append(1.0)\n",
    "    row.append(i)\n",
    "    col.append(itm[1].item_category+1983+9897)\n",
    "    data.append(1.0)\n",
    "\n",
    "basic = coo_matrix((data, (row, col)), shape=(train.shape[0], 1983+9897+3))\n",
    "ext = train.ix[:, 4:].fillna(0).values\n",
    "ext_prob = np.hstack([ext[:, 0:8], ext[:, 12:16], ext[:, 102:142]])\n",
    "ext_nonprob = np.hstack([ext[:, 8:12], ext[:, 16:102], ext[:, 142:]])\n",
    "normalizer = MaxAbsScaler()\n",
    "ext_nonprob = normalizer.fit_transform(ext_nonprob)\n",
    "value = hstack([basic, ext_prob, ext_nonprob])\n",
    "dump_svmlight_file(value, train.rating.values, path+\"libfm/train_0.csv\")\n",
    "\n",
    "data = []\n",
    "row = []\n",
    "col = []\n",
    "for i, itm in enumerate(valid.iterrows()):\n",
    "    row.append(i)\n",
    "    col.append(itm[1].user_id)\n",
    "    data.append(1.0)\n",
    "    row.append(i)\n",
    "    col.append(itm[1].work_id+1983)\n",
    "    data.append(1.0)\n",
    "    row.append(i)\n",
    "    col.append(itm[1].item_category+1983+9897)\n",
    "    data.append(1.0)\n",
    "\n",
    "basic = coo_matrix((data, (row, col)), shape=(valid.shape[0], 1983+9897+3))\n",
    "ext = valid.ix[:, 3:].fillna(0).values\n",
    "ext_prob = np.hstack([ext[:, 0:8], ext[:, 12:16], ext[:, 102:142]])\n",
    "ext_nonprob = np.hstack([ext[:, 8:12], ext[:, 16:102], ext[:, 142:]])\n",
    "ext_nonprob = normalizer.transform(ext_nonprob)\n",
    "value = hstack([basic, ext_prob, ext_nonprob])\n",
    "dump_svmlight_file(value, np.zeros(valid.shape[0]), path+\"libfm/test_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tuser_id\n",
      "1\twork_id\n",
      "2\trating\n",
      "3\titem_category\n",
      "4\tuser_anime_dislike\n",
      "5\tuser_anime_like\n",
      "6\tuser_anime_love\n",
      "7\tuser_anime_neutral\n",
      "8\tuser_manga_dislike\n",
      "9\tuser_manga_like\n",
      "10\tuser_manga_love\n",
      "11\tuser_manga_neutral\n",
      "12\tuser_anime_mean\n",
      "13\tuser_anime_std\n",
      "14\tuser_manga_mean\n",
      "15\tuser_manga_std\n",
      "16\twork_dislike\n",
      "17\twork_like\n",
      "18\twork_love\n",
      "19\twork_neutral\n",
      "20\titem_mean\n",
      "21\titem_std\n",
      "22\titemw2vpos_0_x\n",
      "23\titemw2vpos_1_x\n",
      "24\titemw2vpos_2_x\n",
      "25\titemw2vpos_3_x\n",
      "26\titemw2vpos_4_x\n",
      "27\titemw2vpos_5_x\n",
      "28\titemw2vpos_6_x\n",
      "29\titemw2vpos_7_x\n",
      "30\titemw2vpos_8_x\n",
      "31\titemw2vpos_9_x\n",
      "32\tuserw2vpos_0_x\n",
      "33\tuserw2vpos_1_x\n",
      "34\tuserw2vpos_2_x\n",
      "35\tuserw2vpos_3_x\n",
      "36\tuserw2vpos_4_x\n",
      "37\tuserw2vpos_5_x\n",
      "38\tuserw2vpos_6_x\n",
      "39\tuserw2vpos_7_x\n",
      "40\tuserw2vpos_8_x\n",
      "41\tuserw2vpos_9_x\n",
      "42\titemw2vpos_0_y\n",
      "43\titemw2vpos_1_y\n",
      "44\titemw2vpos_2_y\n",
      "45\titemw2vpos_3_y\n",
      "46\titemw2vpos_4_y\n",
      "47\titemw2vpos_5_y\n",
      "48\titemw2vpos_6_y\n",
      "49\titemw2vpos_7_y\n",
      "50\titemw2vpos_8_y\n",
      "51\titemw2vpos_9_y\n",
      "52\titemw2vpos_10\n",
      "53\titemw2vpos_11\n",
      "54\titemw2vpos_12\n",
      "55\titemw2vpos_13\n",
      "56\titemw2vpos_14\n",
      "57\titemw2vpos_15\n",
      "58\titemw2vpos_16\n",
      "59\titemw2vpos_17\n",
      "60\titemw2vpos_18\n",
      "61\titemw2vpos_19\n",
      "62\titemw2vpos_20\n",
      "63\titemw2vpos_21\n",
      "64\titemw2vpos_22\n",
      "65\titemw2vpos_23\n",
      "66\titemw2vpos_24\n",
      "67\titemw2vpos_25\n",
      "68\titemw2vpos_26\n",
      "69\titemw2vpos_27\n",
      "70\titemw2vpos_28\n",
      "71\titemw2vpos_29\n",
      "72\titemw2vpos_30\n",
      "73\titemw2vpos_31\n",
      "74\tuserw2vpos_0_y\n",
      "75\tuserw2vpos_1_y\n",
      "76\tuserw2vpos_2_y\n",
      "77\tuserw2vpos_3_y\n",
      "78\tuserw2vpos_4_y\n",
      "79\tuserw2vpos_5_y\n",
      "80\tuserw2vpos_6_y\n",
      "81\tuserw2vpos_7_y\n",
      "82\tuserw2vpos_8_y\n",
      "83\tuserw2vpos_9_y\n",
      "84\tuserw2vpos_10\n",
      "85\tuserw2vpos_11\n",
      "86\tuserw2vpos_12\n",
      "87\tuserw2vpos_13\n",
      "88\tuserw2vpos_14\n",
      "89\tuserw2vpos_15\n",
      "90\tuserw2vpos_16\n",
      "91\tuserw2vpos_17\n",
      "92\tuserw2vpos_18\n",
      "93\tuserw2vpos_19\n",
      "94\tuserw2vpos_20\n",
      "95\tuserw2vpos_21\n",
      "96\tuserw2vpos_22\n",
      "97\tuserw2vpos_23\n",
      "98\tuserw2vpos_24\n",
      "99\tuserw2vpos_25\n",
      "100\tuserw2vpos_26\n",
      "101\tuserw2vpos_27\n",
      "102\tuserw2vpos_28\n",
      "103\tuserw2vpos_29\n",
      "104\tuserw2vpos_30\n",
      "105\tuserw2vpos_31\n",
      "106\titem_ldapos_0\n",
      "107\titem_ldapos_1\n",
      "108\titem_ldapos_2\n",
      "109\titem_ldapos_3\n",
      "110\titem_ldapos_4\n",
      "111\titem_ldapos_5\n",
      "112\titem_ldapos_6\n",
      "113\titem_ldapos_7\n",
      "114\titem_ldapos_8\n",
      "115\titem_ldapos_9\n",
      "116\titem_ldapos_10\n",
      "117\titem_ldapos_11\n",
      "118\titem_ldapos_12\n",
      "119\titem_ldapos_13\n",
      "120\titem_ldapos_14\n",
      "121\titem_ldapos_15\n",
      "122\titem_ldapos_16\n",
      "123\titem_ldapos_17\n",
      "124\titem_ldapos_18\n",
      "125\titem_ldapos_19\n",
      "126\tuser_ldapos_0\n",
      "127\tuser_ldapos_1\n",
      "128\tuser_ldapos_2\n",
      "129\tuser_ldapos_3\n",
      "130\tuser_ldapos_4\n",
      "131\tuser_ldapos_5\n",
      "132\tuser_ldapos_6\n",
      "133\tuser_ldapos_7\n",
      "134\tuser_ldapos_8\n",
      "135\tuser_ldapos_9\n",
      "136\tuser_ldapos_10\n",
      "137\tuser_ldapos_11\n",
      "138\tuser_ldapos_12\n",
      "139\tuser_ldapos_13\n",
      "140\tuser_ldapos_14\n",
      "141\tuser_ldapos_15\n",
      "142\tuser_ldapos_16\n",
      "143\tuser_ldapos_17\n",
      "144\tuser_ldapos_18\n",
      "145\tuser_ldapos_19\n",
      "146\twork_lsi_0\n",
      "147\twork_lsi_1\n",
      "148\twork_lsi_2\n",
      "149\twork_lsi_3\n",
      "150\twork_lsi_4\n",
      "151\twork_lsi_5\n",
      "152\twork_lsi_6\n",
      "153\twork_lsi_7\n",
      "154\twork_lsi_8\n",
      "155\twork_lsi_9\n",
      "156\twork_lsi_10\n",
      "157\twork_lsi_11\n",
      "158\twork_lsi_12\n",
      "159\twork_lsi_13\n",
      "160\twork_lsi_14\n",
      "161\twork_lsi_15\n",
      "162\twork_lsi_16\n",
      "163\twork_lsi_17\n",
      "164\twork_lsi_18\n",
      "165\twork_lsi_19\n",
      "166\tuser_lsi_0\n",
      "167\tuser_lsi_1\n",
      "168\tuser_lsi_2\n",
      "169\tuser_lsi_3\n",
      "170\tuser_lsi_4\n",
      "171\tuser_lsi_5\n",
      "172\tuser_lsi_6\n",
      "173\tuser_lsi_7\n",
      "174\tuser_lsi_8\n",
      "175\tuser_lsi_9\n",
      "176\tuser_lsi_10\n",
      "177\tuser_lsi_11\n",
      "178\tuser_lsi_12\n",
      "179\tuser_lsi_13\n",
      "180\tuser_lsi_14\n",
      "181\tuser_lsi_15\n",
      "182\tuser_lsi_16\n",
      "183\tuser_lsi_17\n",
      "184\tuser_lsi_18\n",
      "185\tuser_lsi_19\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train.columns):\n",
    "    print(\"{0}\\t{1}\".format(i, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100015x12056 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13568306 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>work_id</th>\n",
       "      <th>user_dislike</th>\n",
       "      <th>user_like</th>\n",
       "      <th>user_love</th>\n",
       "      <th>user_neutral</th>\n",
       "      <th>user_mean</th>\n",
       "      <th>user_std</th>\n",
       "      <th>work_dislike</th>\n",
       "      <th>work_like</th>\n",
       "      <th>...</th>\n",
       "      <th>user_lsi_10</th>\n",
       "      <th>user_lsi_11</th>\n",
       "      <th>user_lsi_12</th>\n",
       "      <th>user_lsi_13</th>\n",
       "      <th>user_lsi_14</th>\n",
       "      <th>user_lsi_15</th>\n",
       "      <th>user_lsi_16</th>\n",
       "      <th>user_lsi_17</th>\n",
       "      <th>user_lsi_18</th>\n",
       "      <th>user_lsi_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>486</td>\n",
       "      <td>1086</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>2.608696</td>\n",
       "      <td>0.694666</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.225490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050692</td>\n",
       "      <td>-0.449053</td>\n",
       "      <td>0.127121</td>\n",
       "      <td>-0.140252</td>\n",
       "      <td>0.096404</td>\n",
       "      <td>0.083358</td>\n",
       "      <td>0.395873</td>\n",
       "      <td>0.360985</td>\n",
       "      <td>0.810576</td>\n",
       "      <td>-0.536926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1509</td>\n",
       "      <td>3296</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.841379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>2.793103</td>\n",
       "      <td>0.512301</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718754</td>\n",
       "      <td>-1.042719</td>\n",
       "      <td>-0.826651</td>\n",
       "      <td>-0.360414</td>\n",
       "      <td>0.482218</td>\n",
       "      <td>1.215442</td>\n",
       "      <td>-0.891306</td>\n",
       "      <td>-0.507401</td>\n",
       "      <td>0.326895</td>\n",
       "      <td>-0.237996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>617</td>\n",
       "      <td>1086</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>2.805556</td>\n",
       "      <td>0.467177</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.225490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621460</td>\n",
       "      <td>-0.126613</td>\n",
       "      <td>0.096844</td>\n",
       "      <td>0.145862</td>\n",
       "      <td>0.655418</td>\n",
       "      <td>-1.152701</td>\n",
       "      <td>-0.642227</td>\n",
       "      <td>-0.115588</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>-0.146983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>270</td>\n",
       "      <td>9648</td>\n",
       "      <td>0.152597</td>\n",
       "      <td>0.629870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217532</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>0.745979</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535124</td>\n",
       "      <td>0.283546</td>\n",
       "      <td>-0.197112</td>\n",
       "      <td>0.362576</td>\n",
       "      <td>-0.067089</td>\n",
       "      <td>-2.450179</td>\n",
       "      <td>-0.811268</td>\n",
       "      <td>-1.291036</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>0.588306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>459</td>\n",
       "      <td>3647</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>2.419608</td>\n",
       "      <td>0.554516</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272404</td>\n",
       "      <td>-0.392137</td>\n",
       "      <td>0.318417</td>\n",
       "      <td>-0.458198</td>\n",
       "      <td>-0.334445</td>\n",
       "      <td>-0.021697</td>\n",
       "      <td>0.386012</td>\n",
       "      <td>-0.147479</td>\n",
       "      <td>1.022211</td>\n",
       "      <td>-0.370925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  work_id  user_dislike  user_like  user_love  user_neutral  \\\n",
       "0      486     1086      0.119565   0.728261        0.0      0.152174   \n",
       "1     1509     3296      0.048276   0.841379        0.0      0.110345   \n",
       "2      617     1086      0.027778   0.833333        0.0      0.138889   \n",
       "3      270     9648      0.152597   0.629870        0.0      0.217532   \n",
       "4      459     3647      0.031373   0.450980        0.0      0.517647   \n",
       "\n",
       "   user_mean  user_std  work_dislike  work_like     ...       user_lsi_10  \\\n",
       "0   2.608696  0.694666      0.279412   0.225490     ...         -0.050692   \n",
       "1   2.793103  0.512301      0.121212   0.686869     ...          0.718754   \n",
       "2   2.805556  0.467177      0.279412   0.225490     ...          0.621460   \n",
       "3   2.477273  0.745979      0.062500   0.666667     ...          0.535124   \n",
       "4   2.419608  0.554516      0.134146   0.707317     ...          0.272404   \n",
       "\n",
       "   user_lsi_11  user_lsi_12  user_lsi_13  user_lsi_14  user_lsi_15  \\\n",
       "0    -0.449053     0.127121    -0.140252     0.096404     0.083358   \n",
       "1    -1.042719    -0.826651    -0.360414     0.482218     1.215442   \n",
       "2    -0.126613     0.096844     0.145862     0.655418    -1.152701   \n",
       "3     0.283546    -0.197112     0.362576    -0.067089    -2.450179   \n",
       "4    -0.392137     0.318417    -0.458198    -0.334445    -0.021697   \n",
       "\n",
       "   user_lsi_16  user_lsi_17  user_lsi_18  user_lsi_19  \n",
       "0     0.395873     0.360985     0.810576    -0.536926  \n",
       "1    -0.891306    -0.507401     0.326895    -0.237996  \n",
       "2    -0.642227    -0.115588     0.009927    -0.146983  \n",
       "3    -0.811268    -1.291036     0.005630     0.588306  \n",
       "4     0.386012    -0.147479     1.022211    -0.370925  \n",
       "\n",
       "[5 rows x 178 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topic=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_positive = record\n",
    "record_positive_effectiveuser = record_positive[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "record_positive_effectiveuser = record_positive_effectiveuser[record_positive_effectiveuser.work_count>5]\n",
    "record_positive = record_positive[['user_id', 'work_id', 'rating']].groupby(by='user_id').agg(lambda x: list(x))\n",
    "record_positive = pd.concat((record_positive, record_positive_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "corpus = []\n",
    "dictionary = {}\n",
    "cnt=0\n",
    "ratlut = {'love': 1.5, 'like': 1, 'neutral': 0.5, 'dislike':-1}\n",
    "for itm in record_positive.iterrows():\n",
    "    for work in itm[1].work_id:\n",
    "        strwork = 'i{0}'.format(work)\n",
    "        if strwork not in dictionary:\n",
    "            dictionary[strwork]=cnt\n",
    "            cnt+=1\n",
    "    corpus.append([(dictionary['i{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].work_id, itm[1].rating)])\n",
    "invdict = dict(zip(dictionary.values(), dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.lsimodel import LsiModel\n",
    "\n",
    "lsimodel = LsiModel(corpus, num_topics=num_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_positive = record.groupby(by='user_id').agg(lambda x: list(x))\n",
    "\n",
    "corpus = []\n",
    "for itm in record_positive.iterrows():\n",
    "    corpus.append([(dictionary['i{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].work_id, itm[1].rating) if 'i{0}'.format(i) in dictionary])\n",
    "\n",
    "user_lsi_positive = []\n",
    "for i, topic in enumerate(lsimodel[corpus]):\n",
    "    temp = np.zeros(num_topic)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    user_lsi_positive.append(temp)\n",
    "user_lsi_positive = np.vstack(user_lsi_positive)\n",
    "u6 = pd.DataFrame(data = user_lsi_positive, index=record_positive.index, columns=['user_lsi_{0}'.format(i) for i in range(num_topic)])\n",
    "\n",
    "u6.reset_index().to_csv(data+\"features/user_lsi_{0}d.csv\".format(num_topic), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_lsi_0</th>\n",
       "      <th>user_lsi_1</th>\n",
       "      <th>user_lsi_2</th>\n",
       "      <th>user_lsi_3</th>\n",
       "      <th>user_lsi_4</th>\n",
       "      <th>user_lsi_5</th>\n",
       "      <th>user_lsi_6</th>\n",
       "      <th>user_lsi_7</th>\n",
       "      <th>user_lsi_8</th>\n",
       "      <th>user_lsi_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.550102</td>\n",
       "      <td>0.184727</td>\n",
       "      <td>1.420607</td>\n",
       "      <td>-1.484637</td>\n",
       "      <td>-0.104344</td>\n",
       "      <td>-0.957708</td>\n",
       "      <td>0.300948</td>\n",
       "      <td>0.333724</td>\n",
       "      <td>-0.631171</td>\n",
       "      <td>-0.278322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.175376</td>\n",
       "      <td>-0.852548</td>\n",
       "      <td>0.193455</td>\n",
       "      <td>-0.825350</td>\n",
       "      <td>-0.168458</td>\n",
       "      <td>-0.035739</td>\n",
       "      <td>0.353309</td>\n",
       "      <td>-0.141072</td>\n",
       "      <td>1.551080</td>\n",
       "      <td>0.187146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.027113</td>\n",
       "      <td>-0.800671</td>\n",
       "      <td>0.495223</td>\n",
       "      <td>-0.685109</td>\n",
       "      <td>-0.011980</td>\n",
       "      <td>0.846306</td>\n",
       "      <td>0.165598</td>\n",
       "      <td>-0.043316</td>\n",
       "      <td>0.598268</td>\n",
       "      <td>0.641669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.769884</td>\n",
       "      <td>-7.004141</td>\n",
       "      <td>1.018471</td>\n",
       "      <td>0.427918</td>\n",
       "      <td>-1.493721</td>\n",
       "      <td>0.710533</td>\n",
       "      <td>3.962426</td>\n",
       "      <td>-0.102615</td>\n",
       "      <td>2.698168</td>\n",
       "      <td>0.787107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.456074</td>\n",
       "      <td>3.889428</td>\n",
       "      <td>-0.441991</td>\n",
       "      <td>-2.041339</td>\n",
       "      <td>-0.492546</td>\n",
       "      <td>0.685152</td>\n",
       "      <td>-0.064113</td>\n",
       "      <td>-0.539779</td>\n",
       "      <td>-0.639626</td>\n",
       "      <td>-0.064427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_lsi_0  user_lsi_1  user_lsi_2  user_lsi_3  user_lsi_4  \\\n",
       "user_id                                                               \n",
       "0          1.550102    0.184727    1.420607   -1.484637   -0.104344   \n",
       "1          3.175376   -0.852548    0.193455   -0.825350   -0.168458   \n",
       "2          2.027113   -0.800671    0.495223   -0.685109   -0.011980   \n",
       "3          7.769884   -7.004141    1.018471    0.427918   -1.493721   \n",
       "4          5.456074    3.889428   -0.441991   -2.041339   -0.492546   \n",
       "\n",
       "         user_lsi_5  user_lsi_6  user_lsi_7  user_lsi_8  user_lsi_9  \n",
       "user_id                                                              \n",
       "0         -0.957708    0.300948    0.333724   -0.631171   -0.278322  \n",
       "1         -0.035739    0.353309   -0.141072    1.551080    0.187146  \n",
       "2          0.846306    0.165598   -0.043316    0.598268    0.641669  \n",
       "3          0.710533    3.962426   -0.102615    2.698168    0.787107  \n",
       "4          0.685152   -0.064113   -0.539779   -0.639626   -0.064427  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_positive = record\n",
    "record_positive_effectiveitem = record_positive[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "record_positive_effectiveitem = record_positive_effectiveitem[record_positive_effectiveitem.user_count>5]\n",
    "record_positive = record_positive[['user_id', 'work_id', 'rating']].groupby(by='work_id').agg(lambda x: list(x))\n",
    "record_positive = pd.concat((record_positive, record_positive_effectiveitem), axis=1, join='inner')\n",
    "\n",
    "corpus = []\n",
    "dictionary = {}\n",
    "cnt=0\n",
    "ratlut = {'love': 1.5, 'like': 1, 'neutral': 0.5, 'dislike':-1}\n",
    "for itm in record_positive.iterrows():\n",
    "    for user in itm[1].user_id:\n",
    "        struser = 'u{0}'.format(user)\n",
    "        if struser not in dictionary:\n",
    "            dictionary[struser]=cnt\n",
    "            cnt+=1\n",
    "    corpus.append([(dictionary['u{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].user_id, itm[1].rating)])\n",
    "invdict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "lsimodel = LsiModel(corpus, num_topics=num_topic)\n",
    "\n",
    "record_positive = record.groupby(by='work_id').agg(lambda x: list(x))\n",
    "\n",
    "corpus = []\n",
    "for itm in record_positive.iterrows():\n",
    "    corpus.append([(dictionary['u{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].user_id, itm[1].rating) if 'u{0}'.format(i) in dictionary])\n",
    "\n",
    "item_lsi_positive = []\n",
    "for i, topic in enumerate(lsimodel[corpus]):\n",
    "    temp = np.zeros(num_topic)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    item_lsi_positive.append(temp)\n",
    "item_lsi_positive = np.vstack(item_lsi_positive)\n",
    "i6 = pd.DataFrame(data = item_lsi_positive, index=record_positive.index, columns=['work_lsi_{0}'.format(i) for i in range(num_topic)])\n",
    "\n",
    "i6.reset_index().to_csv(data+\"features/item_lsi_{0}d.csv\".format(num_topic), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training set feature (need cv processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>work_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>4041</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>508</td>\n",
       "      <td>1713</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1780</td>\n",
       "      <td>7053</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>658</td>\n",
       "      <td>8853</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>9401</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  work_id  rating  cv\n",
       "0       50     4041       0   3\n",
       "1      508     1713       0   1\n",
       "2     1780     7053       1   2\n",
       "3      658     8853       0   2\n",
       "4     1003     9401       0   3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_user = train_full[['user_id', 'work_id', 'rating']].groupby(by='user_id').agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3371, 9401, 7677, 2799]</td>\n",
       "      <td>[0, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[8359, 9401, 5700, 4633, 1836, 7430, 3398]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[916, 5460, 5434, 2699, 9812]</td>\n",
       "      <td>[0, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[9692, 5616]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[9493, 4041, 2220, 1346, 347]</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            work_id                 rating\n",
       "user_id                                                                   \n",
       "1                          [3371, 9401, 7677, 2799]           [0, 0, 1, 1]\n",
       "2        [8359, 9401, 5700, 4633, 1836, 7430, 3398]  [0, 0, 0, 0, 0, 0, 0]\n",
       "3                     [916, 5460, 5434, 2699, 9812]        [0, 1, 1, 1, 0]\n",
       "4                                      [9692, 5616]                 [0, 1]\n",
       "5                     [9493, 4041, 2220, 1346, 347]        [0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.lsimodel import LsiModel\n",
    "\n",
    "t = train_full\n",
    "train_effectiveuser = t[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "train_effectiveuser = train_effectiveuser[train_effectiveuser.work_count>3]\n",
    "t = t[['user_id', 'work_id', 'rating']].groupby(by='user_id').agg(lambda x: list(x))\n",
    "t = pd.concat((t, train_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "corpus = []\n",
    "dictionary = {}\n",
    "cnt=0\n",
    "\n",
    "for itm in t.iterrows():\n",
    "    for work in itm[1].work_id:\n",
    "        strwork = 'i{0}'.format(work)\n",
    "        if strwork not in dictionary:\n",
    "            dictionary[strwork]=cnt\n",
    "            cnt+=1\n",
    "    corpus.append([(dictionary['i{0}'.format(i)], 2*r-1) for i,r in zip(itm[1].work_id, itm[1].rating)])\n",
    "invdict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "lsimodel = LsiModel(corpus, num_topics=dim)\n",
    "\n",
    "t = train_full[['user_id', 'work_id', 'rating']].groupby(by='user_id').agg(lambda x: list(x))\n",
    "\n",
    "corpus = []\n",
    "for itm in t.iterrows():\n",
    "    corpus.append([(dictionary['i{0}'.format(i)], 2*r-1) for i,r in zip(itm[1].work_id, itm[1].rating) if 'i{0}'.format(i) in dictionary])\n",
    "\n",
    "user_lsi = []\n",
    "for i, topic in enumerate(lsimodel[corpus]):\n",
    "    temp = np.zeros(dim)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    user_lsi.append(temp)\n",
    "user_lsi = np.vstack(user_lsi)\n",
    "u7 = pd.DataFrame(data = user_lsi, index=t.index, columns=['userwish_lsi_{0}'.format(i) for i in range(dim)])\n",
    "\n",
    "u7.reset_index().to_csv(data+\"features/userwish_lsi_{0}d.csv\".format(dim), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cv in [1,2,3]:\n",
    "    train = train_full[~train_full.cv.isin([cv])]\n",
    "    \n",
    "    t = train\n",
    "    train_effectiveuser = t[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "    train_effectiveuser = train_effectiveuser[train_effectiveuser.work_count>3]\n",
    "    t = t[['user_id', 'work_id', 'rating']].groupby(by='user_id').agg(lambda x: list(x))\n",
    "    t = pd.concat((t, train_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "    corpus = []\n",
    "    dictionary = {}\n",
    "    cnt=0\n",
    "\n",
    "    for itm in t.iterrows():\n",
    "        for work in itm[1].work_id:\n",
    "            strwork = 'i{0}'.format(work)\n",
    "            if strwork not in dictionary:\n",
    "                dictionary[strwork]=cnt\n",
    "                cnt+=1\n",
    "        corpus.append([(dictionary['i{0}'.format(i)], 2*r-1) for i,r in zip(itm[1].work_id, itm[1].rating)])\n",
    "    invdict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    lsimodel = LsiModel(corpus, num_topics=dim)\n",
    "\n",
    "    t = train[['user_id', 'work_id', 'rating']].groupby(by='user_id').agg(lambda x: list(x))\n",
    "\n",
    "    corpus = []\n",
    "    for itm in t.iterrows():\n",
    "        corpus.append([(dictionary['i{0}'.format(i)], 2*r-1) for i,r in zip(itm[1].work_id, itm[1].rating) if 'i{0}'.format(i) in dictionary])\n",
    "\n",
    "    user_lsi = []\n",
    "    for i, topic in enumerate(lsimodel[corpus]):\n",
    "        temp = np.zeros(dim)\n",
    "        for itm in topic:\n",
    "            temp[itm[0]] = itm[1]\n",
    "        user_lsi.append(temp)\n",
    "    user_lsi = np.vstack(user_lsi)\n",
    "    u7 = pd.DataFrame(data = user_lsi, index=t.index, columns=['userwish_lsi_{0}'.format(i) for i in range(dim)])\n",
    "\n",
    "    u7.reset_index().to_csv(data+\"features/{1}/userwish_lsi_{0}d.csv\".format(dim, cv), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = train_full\n",
    "train_effectivework = t[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "train_effectivework = train_effectivework[train_effectivework.user_count>3]\n",
    "t = t[['user_id', 'work_id', 'rating']].groupby(by='work_id').agg(lambda x: list(x))\n",
    "t = pd.concat((t, train_effectivework), axis=1, join='inner')\n",
    "\n",
    "corpus = []\n",
    "dictionary = {}\n",
    "cnt=0\n",
    "\n",
    "for itm in t.iterrows():\n",
    "    for user in itm[1].user_id:\n",
    "        struser = 'u{0}'.format(user)\n",
    "        if struser not in dictionary:\n",
    "            dictionary[struser]=cnt\n",
    "            cnt+=1\n",
    "    corpus.append([(dictionary['u{0}'.format(i)], 2*r-1) for i,r in zip(itm[1].user_id, itm[1].rating)])\n",
    "invdict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "lsimodel = LsiModel(corpus, num_topics=dim)\n",
    "\n",
    "t = train_full[['user_id', 'work_id', 'rating']].groupby(by='work_id').agg(lambda x: list(x))\n",
    "\n",
    "corpus = []\n",
    "for itm in t.iterrows():\n",
    "    corpus.append([(dictionary['u{0}'.format(i)], 2*r-1) for i,r in zip(itm[1].user_id, itm[1].rating) if 'u{0}'.format(i) in dictionary])\n",
    "\n",
    "work_lsi = []\n",
    "for i, topic in enumerate(lsimodel[corpus]):\n",
    "    temp = np.zeros(dim)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    work_lsi.append(temp)\n",
    "work_lsi = np.vstack(work_lsi)\n",
    "i7 = pd.DataFrame(data = work_lsi, index=t.index, columns=['itemwish_lsi_{0}'.format(i) for i in range(dim)])\n",
    "\n",
    "i7.reset_index().to_csv(data+\"features/itemwish_lsi_{0}d.csv\".format(dim), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cv in [1,2,3]:\n",
    "    train = train_full[~train_full.cv.isin([cv])]\n",
    "    \n",
    "    t = train\n",
    "    train_effectivework = t[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "    train_effectivework = train_effectivework[train_effectivework.user_count>3]\n",
    "    t = t[['user_id', 'work_id', 'rating']].groupby(by='work_id').agg(lambda x: list(x))\n",
    "    t = pd.concat((t, train_effectivework), axis=1, join='inner')\n",
    "\n",
    "    corpus = []\n",
    "    dictionary = {}\n",
    "    cnt=0\n",
    "\n",
    "    for itm in t.iterrows():\n",
    "        for user in itm[1].user_id:\n",
    "            struser = 'u{0}'.format(user)\n",
    "            if struser not in dictionary:\n",
    "                dictionary[struser]=cnt\n",
    "                cnt+=1\n",
    "        corpus.append([(dictionary['u{0}'.format(i)], 2*r-1) for i,r in zip(itm[1].user_id, itm[1].rating)])\n",
    "    invdict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    lsimodel = LsiModel(corpus, num_topics=dim)\n",
    "\n",
    "    t = train[['user_id', 'work_id', 'rating']].groupby(by='work_id').agg(lambda x: list(x))\n",
    "\n",
    "    corpus = []\n",
    "    for itm in t.iterrows():\n",
    "        corpus.append([(dictionary['u{0}'.format(i)], 2*r-1) for i,r in zip(itm[1].user_id, itm[1].rating) if 'u{0}'.format(i) in dictionary])\n",
    "\n",
    "    work_lsi = []\n",
    "    for i, topic in enumerate(lsimodel[corpus]):\n",
    "        temp = np.zeros(dim)\n",
    "        for itm in topic:\n",
    "            temp[itm[0]] = itm[1]\n",
    "        work_lsi.append(temp)\n",
    "    work_lsi = np.vstack(work_lsi)\n",
    "    i7 = pd.DataFrame(data = work_lsi, index=t.index, columns=['itemwish_lsi_{0}'.format(i) for i in range(dim)])\n",
    "\n",
    "    i7.reset_index().to_csv(data+\"features/{1}/itemwish_lsi_{0}d.csv\".format(dim, cv), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2v feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2vdim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from random import sample\n",
    "\n",
    "t_pos = train_full[train_full.rating==1]\n",
    "train_wish = t_pos[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "train_wish = train_wish[train_wish.work_count>3]\n",
    "t_pos = t_pos[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "t_pos = pd.concat((t_pos, train_wish), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in t_pos.iterrows():\n",
    "    t = ['i{0}'.format(str(i)) for i in itm[1].work_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "itemwish_w2v_positive = []\n",
    "item_index = []\n",
    "for item in range(9897):\n",
    "    if 'i{0}'.format(item) in w2v_positive:\n",
    "        item_index.append(item)\n",
    "        itemwish_w2v_positive.append(w2v_positive['i{0}'.format(item)])\n",
    "itemwish_w2v_positive = np.vstack(itemwish_w2v_positive)\n",
    "i4 = pd.DataFrame(data = itemwish_w2v_positive, index=item_index, columns=['itemwish_w2v_pos_{0}'.format(i) for i in range(w2vdim)])\n",
    "i4.index.name='work_id'\n",
    "\n",
    "i4.reset_index().to_csv(data+\"features/itemwish_w2v_shuffled_{0}d.csv\".format(w2vdim), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cv in [1,2,3]:\n",
    "    train = train_full[~train_full.cv.isin([cv])]\n",
    "    \n",
    "    t_pos = train[train.rating==1]\n",
    "    train_wish = t_pos[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "    train_wish = train_wish[train_wish.work_count>3]\n",
    "    t_pos = t_pos[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "    t_pos = pd.concat((t_pos, train_wish), axis=1, join='inner')\n",
    "\n",
    "    sentences = []\n",
    "    for itm in t_pos.iterrows():\n",
    "        t = ['i{0}'.format(str(i)) for i in itm[1].work_list]\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "\n",
    "    w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "    itemwish_w2v_positive = []\n",
    "    item_index = []\n",
    "    for item in range(9897):\n",
    "        if 'i{0}'.format(item) in w2v_positive:\n",
    "            item_index.append(item)\n",
    "            itemwish_w2v_positive.append(w2v_positive['i{0}'.format(item)])\n",
    "    itemwish_w2v_positive = np.vstack(itemwish_w2v_positive)\n",
    "    i4 = pd.DataFrame(data = itemwish_w2v_positive, index=item_index, columns=['itemwish_w2v_pos_{0}'.format(i) for i in range(w2vdim)])\n",
    "    i4.index.name='work_id'\n",
    "\n",
    "    i4.reset_index().to_csv(data+\"features/{1}/itemwish_w2v_shuffled_{0}d.csv\".format(w2vdim, cv), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_pos = train_full[train_full.rating==1]\n",
    "train_wish = t_pos[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "train_wish = train_wish[train_wish.user_count>3]\n",
    "t_pos = t_pos[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "t_pos = pd.concat((t_pos, train_wish), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in t_pos.iterrows():\n",
    "    t = ['u{0}'.format(str(i)) for i in itm[1].user_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "userwish_w2v_positive = []\n",
    "user_index = []\n",
    "for user in range(1983):\n",
    "    if 'u{0}'.format(user) in w2v_positive:\n",
    "        user_index.append(user)\n",
    "        userwish_w2v_positive.append(w2v_positive['u{0}'.format(user)])\n",
    "userwish_w2v_positive = np.vstack(userwish_w2v_positive)\n",
    "u4 = pd.DataFrame(data = userwish_w2v_positive, index=user_index, columns=['userwish_w2v_pos_{0}'.format(i) for i in range(w2vdim)])\n",
    "u4.index.name='user_id'\n",
    "\n",
    "u4.reset_index().to_csv(data+\"features/userwish_w2v_shuffled_{0}d.csv\".format(w2vdim), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cv in [1,2,3]:\n",
    "    train = train_full[~train_full.cv.isin([cv])]\n",
    "    \n",
    "    t_pos = train[train.rating==1]\n",
    "    train_wish = t_pos[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "    train_wish = train_wish[train_wish.user_count>3]\n",
    "    t_pos = t_pos[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "    t_pos = pd.concat((t_pos, train_wish), axis=1, join='inner')\n",
    "\n",
    "    sentences = []\n",
    "    for itm in t_pos.iterrows():\n",
    "        t = ['u{0}'.format(str(i)) for i in itm[1].user_list]\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "\n",
    "    w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "    userwish_w2v_positive = []\n",
    "    user_index = []\n",
    "    for user in range(1983):\n",
    "        if 'u{0}'.format(user) in w2v_positive:\n",
    "            user_index.append(user)\n",
    "            userwish_w2v_positive.append(w2v_positive['u{0}'.format(user)])\n",
    "    userwish_w2v_positive = np.vstack(userwish_w2v_positive)\n",
    "    u4 = pd.DataFrame(data = userwish_w2v_positive, index=user_index, columns=['userwish_w2v_pos_{0}'.format(i) for i in range(w2vdim)])\n",
    "    u4.index.name='user_id'\n",
    "\n",
    "    u4.reset_index().to_csv(data+\"features/{1}/userwish_w2v_shuffled_{0}d.csv\".format(w2vdim,cv), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_pos = train_full[train_full.rating==0]\n",
    "train_wish = t_pos[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "train_wish = train_wish[train_wish.work_count>3]\n",
    "t_pos = t_pos[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "t_pos = pd.concat((t_pos, train_wish), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in t_pos.iterrows():\n",
    "    t = ['i{0}'.format(str(i)) for i in itm[1].work_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "itemwish_w2v_positive = []\n",
    "item_index = []\n",
    "for item in range(9897):\n",
    "    if 'i{0}'.format(item) in w2v_positive:\n",
    "        item_index.append(item)\n",
    "        itemwish_w2v_positive.append(w2v_positive['i{0}'.format(item)])\n",
    "itemwish_w2v_positive = np.vstack(itemwish_w2v_positive)\n",
    "i4 = pd.DataFrame(data = itemwish_w2v_positive, index=item_index, columns=['itemwish_w2v_neg_{0}'.format(i) for i in range(w2vdim)])\n",
    "i4.index.name='work_id'\n",
    "\n",
    "i4.reset_index().to_csv(data+\"features/itemwish_w2vneg_shuffled_{0}d.csv\".format(w2vdim), index=False)\n",
    "\n",
    "for cv in [1,2,3]:\n",
    "    train = train_full[~train_full.cv.isin([cv])]\n",
    "    \n",
    "    t_pos = train[train.rating==0]\n",
    "    train_wish = t_pos[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "    train_wish = train_wish[train_wish.work_count>3]\n",
    "    t_pos = t_pos[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "    t_pos = pd.concat((t_pos, train_wish), axis=1, join='inner')\n",
    "\n",
    "    sentences = []\n",
    "    for itm in t_pos.iterrows():\n",
    "        t = ['i{0}'.format(str(i)) for i in itm[1].work_list]\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "\n",
    "    w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "    itemwish_w2v_positive = []\n",
    "    item_index = []\n",
    "    for item in range(9897):\n",
    "        if 'i{0}'.format(item) in w2v_positive:\n",
    "            item_index.append(item)\n",
    "            itemwish_w2v_positive.append(w2v_positive['i{0}'.format(item)])\n",
    "    itemwish_w2v_positive = np.vstack(itemwish_w2v_positive)\n",
    "    i4 = pd.DataFrame(data = itemwish_w2v_positive, index=item_index, columns=['itemwish_w2v_neg_{0}'.format(i) for i in range(w2vdim)])\n",
    "    i4.index.name='work_id'\n",
    "\n",
    "    i4.reset_index().to_csv(data+\"features/{1}/itemwish_w2vneg_shuffled_{0}d.csv\".format(w2vdim, cv), index=False)\n",
    "    \n",
    "t_pos = train_full[train_full.rating==0]\n",
    "train_wish = t_pos[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "train_wish = train_wish[train_wish.user_count>3]\n",
    "t_pos = t_pos[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "t_pos = pd.concat((t_pos, train_wish), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in t_pos.iterrows():\n",
    "    t = ['u{0}'.format(str(i)) for i in itm[1].user_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "userwish_w2v_positive = []\n",
    "user_index = []\n",
    "for user in range(1983):\n",
    "    if 'u{0}'.format(user) in w2v_positive:\n",
    "        user_index.append(user)\n",
    "        userwish_w2v_positive.append(w2v_positive['u{0}'.format(user)])\n",
    "userwish_w2v_positive = np.vstack(userwish_w2v_positive)\n",
    "u4 = pd.DataFrame(data = userwish_w2v_positive, index=user_index, columns=['userwish_w2v_neg_{0}'.format(i) for i in range(w2vdim)])\n",
    "u4.index.name='user_id'\n",
    "\n",
    "u4.reset_index().to_csv(data+\"features/userwish_w2vneg_shuffled_{0}d.csv\".format(w2vdim), index=False)\n",
    "\n",
    "for cv in [1,2,3]:\n",
    "    train = train_full[~train_full.cv.isin([cv])]\n",
    "    \n",
    "    t_pos = train[train.rating==0]\n",
    "    train_wish = t_pos[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "    train_wish = train_wish[train_wish.user_count>3]\n",
    "    t_pos = t_pos[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "    t_pos = pd.concat((t_pos, train_wish), axis=1, join='inner')\n",
    "\n",
    "    sentences = []\n",
    "    for itm in t_pos.iterrows():\n",
    "        t = ['u{0}'.format(str(i)) for i in itm[1].user_list]\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "        sentences.append(sample(t, len(t)))\n",
    "\n",
    "    w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "    userwish_w2v_positive = []\n",
    "    user_index = []\n",
    "    for user in range(1983):\n",
    "        if 'u{0}'.format(user) in w2v_positive:\n",
    "            user_index.append(user)\n",
    "            userwish_w2v_positive.append(w2v_positive['u{0}'.format(user)])\n",
    "    userwish_w2v_positive = np.vstack(userwish_w2v_positive)\n",
    "    u4 = pd.DataFrame(data = userwish_w2v_positive, index=user_index, columns=['userwish_w2v_neg_{0}'.format(i) for i in range(w2vdim)])\n",
    "    u4.index.name='user_id'\n",
    "\n",
    "    u4.reset_index().to_csv(data+\"features/{1}/userwish_w2vneg_shuffled_{0}d.csv\".format(w2vdim,cv), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
