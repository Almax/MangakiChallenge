{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = \"/mnt/d/Data/mangaki-data-challenge/\"\n",
    "\n",
    "record = pd.read_csv(data+'watched.csv', dtype={\n",
    "    'user_id': np.int16,\n",
    "    'work_id': np.int16,\n",
    "    'rating': 'category'\n",
    "})\n",
    "\n",
    "train_full = pd.read_csv(data+'train_withcv.csv', dtype={\n",
    "    'user_id': np.int16,\n",
    "    'work_id': np.int16,\n",
    "    'rating': np.int8,\n",
    "    'cv': np.int8\n",
    "})\n",
    "\n",
    "test = pd.read_csv(data+'test.csv', dtype={\n",
    "    'user_id': np.int16,\n",
    "    'work_id': np.int16\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u1 = pd.crosstab(record.user_id, record.rating).add_prefix('user_').apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratelut = {\n",
    "    'dislike':1.0,\n",
    "    'like':3.0,\n",
    "    'love':4.0,\n",
    "    'neutral':2.0\n",
    "}\n",
    "record['score']=record.rating.map(lambda x: ratelut[x]).astype(float)\n",
    "u2 = record[['user_id', 'score']].groupby(by='user_id')['score'].agg(['mean', 'std']).rename(columns={'mean':'user_mean', 'std':'user_std'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i1 = pd.crosstab(record.work_id, record.rating).add_prefix('work_').apply(lambda r: r/r.sum(), axis=1)\n",
    "i2 = record[['work_id', 'score']].groupby(by='work_id')['score'].agg(['mean', 'std']).add_prefix('item_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deprecated\n",
    "valid = train_full[train_full.cv==3]\n",
    "train = train_full[~train_full.cv.isin([3])]\n",
    "u3 = pd.crosstab(train.user_id, train.rating).add_prefix('user_').apply(lambda r: r/r.sum(), axis=1)\n",
    "i3 = pd.crosstab(train.work_id, train.rating).add_prefix('item_').apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3754, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cv in [1,2,3]:\n",
    "    valid = train_full[train_full.cv==cv]\n",
    "    train = train_full[~train_full.cv.isin([cv])]\n",
    "    train = train.merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "                  merge(i2, left_on='work_id', right_index=True, how='left')\n",
    "    valid = valid.merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "                  merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "                  merge(i2, left_on='work_id', right_index=True, how='left')\n",
    "    train = train.drop(['cv'], axis=1)\n",
    "    train.to_csv(data+\"train_{0}.csv\".format(cv), index=False)\n",
    "    valid = valid.drop(['cv'], axis=1)\n",
    "    valid.to_csv(data+\"valid_{0}.csv\".format(cv), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "merge(i2, left_on='work_id', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['cv'], axis=1)\n",
    "train.to_csv(data+\"train_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid = valid.merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "merge(i2, left_on='work_id', right_index=True, how='left')\n",
    "valid = valid.drop(['cv'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid.to_csv(data+\"valid_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_full\n",
    "#u3 = pd.crosstab(train.user_id, train.rating).add_prefix('user_').apply(lambda r: r/r.sum(), axis=1)\n",
    "#i3 = pd.crosstab(train.work_id, train.rating).add_prefix('item_').apply(lambda r: r/r.sum(), axis=1)\n",
    "train = train.merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "merge(i2, left_on='work_id', right_index=True, how='left')\n",
    "train = train.drop(['cv'], axis=1)\n",
    "train.to_csv(data+\"baseline/train_0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.merge(u1, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(u2, left_on='user_id', right_index=True, how='left').\\\n",
    "merge(i1, left_on='work_id', right_index=True, how='left').\\\n",
    "merge(i2, left_on='work_id', right_index=True, how='left')\n",
    "test.to_csv(data+\"baseline/test_0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "W2V feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2vdim=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from random import sample\n",
    "\n",
    "record_positive = record[record.rating.isin(['like', 'love'])]\n",
    "record_positive_effectiveuser = record_positive[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "record_positive_effectiveuser = record_positive_effectiveuser[record_positive_effectiveuser.work_count>5]\n",
    "record_positive = record_positive[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "record_positive = pd.concat((record_positive, record_positive_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_positive.iterrows():\n",
    "    t = ['i{0}'.format(str(i)) for i in itm[1].work_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "item_w2v_positive = []\n",
    "item_index = []\n",
    "for item in range(9897):\n",
    "    if 'i{0}'.format(item) in w2v_positive:\n",
    "        item_index.append(item)\n",
    "        item_w2v_positive.append(w2v_positive['i{0}'.format(item)])\n",
    "item_w2v_positive = np.vstack(item_w2v_positive)\n",
    "i4 = pd.DataFrame(data = item_w2v_positive, index=item_index, columns=['itemw2vpos_{0}'.format(i) for i in range(w2vdim)])\n",
    "i4.index.name='work_id'\n",
    "\n",
    "i4.reset_index().to_csv(data+\"features/item_w2v_shuffled_{0}d.csv\".format(w2vdim), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for user\n",
    "record_positive = record[record.rating.isin(['like', 'love'])]\n",
    "record_positive_effectiveitem = record_positive[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "record_positive_effectiveitem = record_positive_effectiveitem[record_positive_effectiveitem.user_count>5]\n",
    "record_positive = record_positive[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "record_positive = pd.concat((record_positive, record_positive_effectiveitem), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_positive.iterrows():\n",
    "    t = ['u{0}'.format(str(i)) for i in itm[1].user_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_positive = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "user_w2v_positive = []\n",
    "user_index = []\n",
    "for user in range(1983):\n",
    "    if 'u{0}'.format(user) in w2v_positive:\n",
    "        user_index.append(user)\n",
    "        user_w2v_positive.append(w2v_positive['u{0}'.format(user)])\n",
    "user_w2v_positive = np.vstack(user_w2v_positive)\n",
    "u4 = pd.DataFrame(data = user_w2v_positive, index=user_index, columns=['userw2vpos_{0}'.format(i) for i in range(w2vdim)])\n",
    "u4.index.name='user_id'\n",
    "\n",
    "u4.reset_index().to_csv(data+\"features/user_w2v_shuffled_{0}d.csv\".format(w2vdim), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "record_negative = record[record.rating.isin(['dislike'])]\n",
    "record_negative_effectiveuser = record_negative[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "record_negative_effectiveuser = record_negative_effectiveuser[record_negative_effectiveuser.work_count>5]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "record_negative = pd.concat((record_negative, record_negative_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    t = ['i{0}'.format(str(i)) for i in itm[1].work_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_negative = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "item_w2v_negative = []\n",
    "item_index = []\n",
    "for item in range(9897):\n",
    "    if 'i{0}'.format(item) in w2v_negative:\n",
    "        item_index.append(item)\n",
    "        item_w2v_negative.append(w2v_negative['i{0}'.format(item)])\n",
    "item_w2v_negative = np.vstack(item_w2v_negative)\n",
    "i4 = pd.DataFrame(data = item_w2v_negative, index=item_index, columns=['itemw2vpos_{0}'.format(i) for i in range(w2vdim)])\n",
    "i4.index.name='work_id'\n",
    "\n",
    "i4.reset_index().to_csv(data+\"features/item_w2vneg_shuffled_{0}d.csv\".format(w2vdim), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_negative = record[record.rating.isin(['dislike'])]\n",
    "record_negative_effectiveitem = record_negative[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "record_negative_effectiveitem = record_negative_effectiveitem[record_negative_effectiveitem.user_count>5]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "record_negative = pd.concat((record_negative, record_negative_effectiveitem), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    t = ['u{0}'.format(str(i)) for i in itm[1].user_list]\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    sentences.append(sample(t, len(t)))\n",
    "    \n",
    "w2v_negative = Word2Vec(sentences, size=w2vdim)\n",
    "\n",
    "user_w2v_negative = []\n",
    "user_index = []\n",
    "for user in range(1983):\n",
    "    if 'u{0}'.format(user) in w2v_negative:\n",
    "        user_index.append(user)\n",
    "        user_w2v_negative.append(w2v_negative['u{0}'.format(user)])\n",
    "user_w2v_negative = np.vstack(user_w2v_negative)\n",
    "u4 = pd.DataFrame(data = user_w2v_negative, index=user_index, columns=['userw2vpos_{0}'.format(i) for i in range(w2vdim)])\n",
    "u4.index.name='user_id'\n",
    "\n",
    "u4.reset_index().to_csv(data+\"features/user_w2vneg_shuffled_{0}d.csv\".format(w2vdim), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LDA feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "record_positive = record[record.rating.isin(['like', 'love', 'neutral'])]\n",
    "record_positive_effectiveuser = record_positive[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "record_positive_effectiveuser = record_positive_effectiveuser[record_positive_effectiveuser.work_count>5]\n",
    "record_positive = record_positive[['user_id', 'work_id', 'rating']].groupby(by='user_id').agg(lambda x: list(x))\n",
    "record_positive = pd.concat((record_positive, record_positive_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "corpus = []\n",
    "dictionary = {}\n",
    "cnt=0\n",
    "ratlut = {'love': 1.5, 'like': 1, 'neutral': 0.5}\n",
    "for itm in record_positive.iterrows():\n",
    "    for work in itm[1].work_id:\n",
    "        strwork = 'i{0}'.format(work)\n",
    "        if strwork not in dictionary:\n",
    "            dictionary[strwork]=cnt\n",
    "            cnt+=1\n",
    "    corpus.append([(dictionary['i{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].work_id, itm[1].rating)])\n",
    "invdict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "lda_user = LdaModel(corpus=corpus, num_topics=num_topics, id2word=invdict)\n",
    "\n",
    "record_positive = record[record.rating.isin(['like', 'love', 'neutral'])]\n",
    "record_positive = record_positive[['user_id', 'work_id', 'rating']].groupby(by='user_id').agg(lambda x: list(x))\n",
    "\n",
    "corpus = []\n",
    "for itm in record_positive.iterrows():\n",
    "    corpus.append([(dictionary['i{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].work_id, itm[1].rating) if 'i{0}'.format(i) in dictionary])\n",
    "\n",
    "user_lda_positive = []\n",
    "for i, topic in enumerate(lda_user[corpus]):\n",
    "    temp = np.zeros(num_topics)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    user_lda_positive.append(temp)\n",
    "user_lda_positive = np.vstack(user_lda_positive)\n",
    "u5 = pd.DataFrame(data = user_lda_positive, index=record_positive.index, columns=['user_ldapos_{0}'.format(i) for i in range(num_topics)])\n",
    "\n",
    "u5.reset_index().to_csv(data+\"features/user_lda_{0}d.csv\".format(num_topics), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_positive = record[record.rating.isin(['like', 'love', 'neutral'])]\n",
    "record_positive_effectiveitem = record_positive[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "record_positive_effectiveitem = record_positive_effectiveitem[record_positive_effectiveitem.user_count>5]\n",
    "record_positive = record_positive[['user_id', 'work_id', 'rating']].groupby(by='work_id').agg(lambda x: list(x))\n",
    "record_positive = pd.concat((record_positive, record_positive_effectiveitem), axis=1, join='inner')\n",
    "\n",
    "corpus = []\n",
    "dictionary = {}\n",
    "cnt=0\n",
    "ratlut = {'love': 1.5, 'like': 1, 'neutral': 0.5}\n",
    "for itm in record_positive.iterrows():\n",
    "    for user in itm[1].user_id:\n",
    "        struser = 'u{0}'.format(user)\n",
    "        if struser not in dictionary:\n",
    "            dictionary[struser]=cnt\n",
    "            cnt+=1\n",
    "    corpus.append([(dictionary['u{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].user_id, itm[1].rating)])\n",
    "invdict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "lda_item = LdaModel(corpus=corpus, num_topics=num_topics, id2word=invdict)\n",
    "\n",
    "record_positive = record[record.rating.isin(['like', 'love', 'neutral'])]\n",
    "record_positive = record_positive[['user_id', 'work_id', 'rating']].groupby(by='work_id').agg(lambda x: list(x))\n",
    "\n",
    "corpus = []\n",
    "for itm in record_positive.iterrows():\n",
    "    corpus.append([(dictionary['u{0}'.format(i)], ratlut[r]) for i,r in zip(itm[1].user_id, itm[1].rating) if 'u{0}'.format(i) in dictionary])\n",
    "\n",
    "item_lda_positive = []\n",
    "for i, topic in enumerate(lda_item[corpus]):\n",
    "    temp = np.zeros(num_topics)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    item_lda_positive.append(temp)\n",
    "item_lda_positive = np.vstack(item_lda_positive)\n",
    "i5 = pd.DataFrame(data = item_lda_positive, index=record_positive.index, columns=['item_ldapos_{0}'.format(i) for i in range(num_topics)])\n",
    "\n",
    "i5.reset_index().to_csv(data+\"features/item_lda_{0}d.csv\".format(num_topics), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "\n",
    "record_negative = record[record.rating.isin(['dislike'])]\n",
    "record_negative_effectiveuser = record_negative[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_count': 'count'})\n",
    "record_negative_effectiveuser = record_negative_effectiveuser[record_negative_effectiveuser.work_count>5]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "record_negative = pd.concat((record_negative, record_negative_effectiveuser), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    sentences.append(['i{0}'.format(str(i)) for i in itm[1].work_list])\n",
    "    \n",
    "dictionary = corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(rc) for rc in sentences]\n",
    "\n",
    "lda_user = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "record_negative = record[record.rating.isin(['dislike'])]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='user_id')['work_id'].agg({'work_list': lambda x: list(x)})\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    sentences.append(['i{0}'.format(str(i)) for i in itm[1].work_list])\n",
    "corpus = [dictionary.doc2bow(rc) for rc in sentences]\n",
    "\n",
    "user_lda_negative = []\n",
    "for i, topic in enumerate(lda_user[corpus]):\n",
    "    temp = np.zeros(num_topics)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    user_lda_negative.append(temp)\n",
    "user_lda_negative = np.vstack(user_lda_negative)\n",
    "u5 = pd.DataFrame(data = user_lda_negative, index=record_negative.index, columns=['user_ldapos_{0}'.format(i) for i in range(num_topics)])\n",
    "\n",
    "u5.reset_index().to_csv(data+\"features/user_ldaneg_{0}d.csv\".format(num_topics), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_negative = record[record.rating.isin(['dislike'])]\n",
    "record_negative_effectiveitem = record_negative[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_count': 'count'})\n",
    "record_negative_effectiveitem = record_negative_effectiveitem[record_negative_effectiveitem.user_count>5]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "record_negative = pd.concat((record_negative, record_negative_effectiveitem), axis=1, join='inner')\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    sentences.append(['u{0}'.format(str(i)) for i in itm[1].user_list])\n",
    "    \n",
    "dictionary = corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(rc) for rc in sentences]\n",
    "\n",
    "lda_item = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "record_negative = record[record.rating.isin(['like', 'love'])]\n",
    "record_negative = record_negative[['user_id', 'work_id']].groupby(by='work_id')['user_id'].agg({'user_list': lambda x: list(x)})\n",
    "\n",
    "sentences = []\n",
    "for itm in record_negative.iterrows():\n",
    "    sentences.append(['u{0}'.format(str(i)) for i in itm[1].user_list])\n",
    "corpus = [dictionary.doc2bow(rc) for rc in sentences]\n",
    "\n",
    "item_lda_negative = []\n",
    "for i, topic in enumerate(lda_item[corpus]):\n",
    "    temp = np.zeros(num_topics)\n",
    "    for itm in topic:\n",
    "        temp[itm[0]] = itm[1]\n",
    "    item_lda_negative.append(temp)\n",
    "item_lda_negative = np.vstack(item_lda_negative)\n",
    "i5 = pd.DataFrame(data = item_lda_negative, index=record_negative.index, columns=['item_ldapos_{0}'.format(i) for i in range(num_topics)])\n",
    "\n",
    "i5.reset_index().to_csv(data+\"features/item_ldaneg_{0}d.csv\".format(num_topics), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "convert to libfm format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "\n",
    "path = \"/mnt/d/Data/mangaki-data-challenge/\"\n",
    "for cv in [1,2,3]:\n",
    "    train = pd.read_csv(path+\"baseline/train_{0}.csv\".format(cv))\n",
    "    valid = pd.read_csv(path+\"baseline/valid_{0}.csv\".format(cv))\n",
    "    \n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    for i, itm in enumerate(train.iterrows()):\n",
    "        row.append(i)\n",
    "        col.append(itm[1].user_id)\n",
    "        data.append(1.0)\n",
    "        row.append(i)\n",
    "        col.append(itm[1].work_id+1983)\n",
    "        data.append(1.0)\n",
    "\n",
    "    basic = coo_matrix((data, (row, col)), shape=(train.shape[0], 1983+9897))\n",
    "    ext = train.ix[:, 3:].fillna(0).values\n",
    "    ext[:, 4] = ext[:, 4]/4\n",
    "    ext[:, 10] = ext[:, 10]/4\n",
    "    value = hstack([basic, ext])\n",
    "    dump_svmlight_file(value, train.rating.values, path+\"libfm/train_{0}.csv\".format(cv))\n",
    "    \n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    for i, itm in enumerate(valid.iterrows()):\n",
    "        row.append(i)\n",
    "        col.append(itm[1].user_id)\n",
    "        data.append(1.0)\n",
    "        row.append(i)\n",
    "        col.append(itm[1].work_id+1983)\n",
    "        data.append(1.0)\n",
    "\n",
    "    basic = coo_matrix((data, (row, col)), shape=(valid.shape[0], 1983+9897))\n",
    "    ext = valid.ix[:, 3:].fillna(0).values\n",
    "    ext[:, 4] = ext[:, 4]/4\n",
    "    ext[:, 10] = ext[:, 10]/4\n",
    "    value = hstack([basic, ext])\n",
    "    dump_svmlight_file(value, valid.rating.values, path+\"libfm/valid_{0}.csv\".format(cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+\"baseline/train_0.csv\")\n",
    "valid = pd.read_csv(path+\"baseline/test_0.csv\")\n",
    "\n",
    "data = []\n",
    "row = []\n",
    "col = []\n",
    "for i, itm in enumerate(train.iterrows()):\n",
    "    row.append(i)\n",
    "    col.append(itm[1].user_id)\n",
    "    data.append(1.0)\n",
    "    row.append(i)\n",
    "    col.append(itm[1].work_id+1983)\n",
    "    data.append(1.0)\n",
    "\n",
    "basic = coo_matrix((data, (row, col)), shape=(train.shape[0], 1983+9897))\n",
    "ext = train.ix[:, 3:].fillna(0).values\n",
    "ext[:, 4] = ext[:, 4]/4\n",
    "ext[:, 10] = ext[:, 10]/4\n",
    "value = hstack([basic, ext])\n",
    "dump_svmlight_file(value, train.rating.values, path+\"libfm/train_0.csv\")\n",
    "\n",
    "data = []\n",
    "row = []\n",
    "col = []\n",
    "for i, itm in enumerate(valid.iterrows()):\n",
    "    row.append(i)\n",
    "    col.append(itm[1].user_id)\n",
    "    data.append(1.0)\n",
    "    row.append(i)\n",
    "    col.append(itm[1].work_id+1983)\n",
    "    data.append(1.0)\n",
    "\n",
    "basic = coo_matrix((data, (row, col)), shape=(valid.shape[0], 1983+9897))\n",
    "ext = valid.ix[:, 3:].fillna(0).values\n",
    "ext[:, 4] = ext[:, 4]/4\n",
    "ext[:, 10] = ext[:, 10]/4\n",
    "value = hstack([basic, ext])\n",
    "dump_svmlight_file(value, np.zeros(valid.shape[0]), path+\"libfm/test_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
